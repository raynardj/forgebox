# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/15_loopstack.ipynb (unless otherwise specified).

__all__ = ['create_event', 'events', 'LoopStack', 'train_callbacks', 'to_tensor', 'train_single_forward', 'TrainLoop',
           'EvalLoop']

# Cell
from .loop import Loop,ProgressBar,Tolerate,Event,Stuff,chunkify
from types import MethodType
import numpy as np

# Cell
def create_event(event_name):
    class BatchEvent(Event):pass
    BatchEvent.__name__ = event_name
    return BatchEvent

def events(*enames):
    return list(map(create_event,enames))

# Cell
class LoopStack(Loop):
    settings = []
    """
    A stack of loop
    """
    @classmethod
    def from_loops(cls,*loops):
        def init(self,iterable=[],name = None):
            name = name if name!=None else cls.__name__

            self.loops = dict()
            l = Loop(iterable)
            for L in loops:
                l = L(iterable = l)
            super().__init__(iterable = l,
                             name = name)

            for stuff in cls.settings:
                self.make_stuff(stuff)

        setattr(cls,"init",init)
        return cls

    @classmethod
    def new_setting(cls,*settings):
        cls.settings+=list(settings)

    def make_stuff(self,name):
        new_stuff = Stuff(name)
        setattr(self.core,name,new_stuff)
        setattr(self,name,new_stuff)

    def __repr__(self,):
        return f"LoopStack>:{self.name}\n\t"+\
            "\n\t".join(map(str,self.core.layers[:-1]))

# Cell
import torch

def train_callbacks(loop):
    @loop.on_DATA_PROCESS
    def opt_zero_grad(loop):
        loop.opt("zero_grad")

    @loop.before_1st_FORWARD
    def switch_model_to_train(loop):
        loop.model("train")

    @loop.BACKWARD.on
    def opt_step(loop):
        loop.loss("backward")

    @loop.BACKWARD.on
    def opt_step(loop):
        loop.opt("step")

def to_tensor(x):
    return torch.Tensor(x)

def train_single_forward(metric_func = []):
    def train_single_forward_cb(loop):
        @loop.on_DATA_PROCESS
        def set_xy(loop):
            loop.x["x1"],loop.y["y1"] = loop.element
            loop.x.update(to_tensor)
            loop.y.update(to_tensor)

        @loop.on_FORWARD
        def forward_pass(loop):
            loop.core.pred = loop.model("__call__",loop.x.x1)[0]

        @loop.on_LOSS_CALC
        def calculate_loss(loop):
            loop.loss["latest"] = loop.loss_func("__call__",loop.pred,loop.y.y1)

        @loop.on_METRICS
        def calcualte_metrics(loop):
            # calculate metrics
            with torch.no_grad():
                for func in metric_func:
                    loop.metric[func.__name__] = func(loop.pred,loop.y)

            # loop through metrics
            loop.pgbar_data(loop.metric.cases)

    return train_single_forward_cb

class TrainLoop(LoopStack):
    def __init__(self,data_iter,model=[],opt=[],loss_func=[],loss=[],hp=[],cuda=[],
                 callbacks = [train_callbacks,],tolerate=True):
        loops = [ProgressBar,]
        if tolerate:
            loops.append(Tolerate)
        loops+=list(events(*TRAIN_EVENTS))
        self.from_loops(*loops)
        self.new_setting("model","x","y",
                         "opt","loss_func","loss",
                         "hp","cuda","metric_func","metric")
        self.init(data_iter,)
        for cb in callbacks:
            print(f"assigning callback {cb}")
            cb(self)

class EvalLoop(LoopStack):
    def __init__(self,data_iter,tolerate=True):
        loops = [ProgressBar,]
        if tolerate:
            loops.append(Tolerate)
        loops+=list(events(*EVAL_EVENTS))
        self.from_loops(*loops)
        self.new_setting("model","opt","loss","hp","cuda")
        self.__init__(data_iter,)

        @self.EVAL_FORWARD.downstream
        def torch_eval_wrap(self,func):
            with torch.no_grad():
                func()