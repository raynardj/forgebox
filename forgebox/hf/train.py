# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/72_pl_training.ipynb (unless otherwise specified).

__all__ = ['ner_model_from', 'ner_tokenizer_from', 'NERDataModule', 'NERModule', 'clean_ner_output',
           'predict_ner_table', 'refined_ner_pipeline', 'refined_ner_from_pretrained']

# Cell
from .data import IOBES
from ..imports import *
import pytorch_lightning as pl
from transformers import (
    AutoModelForTokenClassification,
    AutoTokenizer,
    pipeline
)
from typing import Callable

# Cell

# ner model and tokenizer
def ner_model_from(
    name:str, dataset: IOBES
):
    """
    name: from_pretrain(name)
    """
    model = AutoModelForTokenClassification.from_pretrained(
        name,
        num_labels=len(dataset.cates),
    )
    dataset.set_hfconfig(model.config)
    return model

def ner_tokenizer_from(
    name: str
):
    return AutoTokenizer.from_pretrained(
        name, add_prefix_space=True)

# Cell

# ner data module
class NERDataModule(pl.LightningDataModule):
    def __init__(self, train_ds, val_ds, batch_size=32):
        super().__init__()
        self.train_ds = train_ds
        self.val_ds = val_ds
        self.batch_size = batch_size

    def train_dataloader(self):
        return self.train_ds.dataloader(batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return self.val_ds.dataloader(batch_size=self.batch_size*2, shuffle=False)

# Cell

# ner module
class NERModule(pl.LightningModule):
    """
    PyTorch lightning module for training ner model
    """
    def __init__(
        self, model,
        ):
        """
        model: huggingface transformer model for ner
        """
        super().__init__()
        self.model = model

    def forward(self, batch):
        return self.model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['labels'])

    def training_step(self, batch, batch_idx):
        outputs = self(batch)
        loss = outputs.loss
        self.log("loss", loss)
        self.log("acc", self.calcualte_acc(outputs, batch.labels))
        return loss

    def validation_step(self, batch, batch_idx):
        outputs = self(batch)
        loss = outputs.loss
        self.log("val_loss", loss)
        self.log("val_acc", self.calcualte_acc(outputs, batch.labels))
        return loss

    def calcualte_acc(self, outputs, labels):
        pred_idx = outputs.logits.argmax(-1)
        mask = torch.ones_like(pred_idx)
        mask[labels==-100]=False
        return (pred_idx[mask]==labels[mask]).float().mean()

    def configure_optimizers(self):
        # discriminative learning rate
        param_groups = [
            {'params': self.model.roberta.parameters(), 'lr': 5e-6},
            {'params': self.model.classifier.parameters(), 'lr': 1e-3},
        ]
        optimizer = torch.optim.Adam(param_groups, lr=1e-3)
        return optimizer

# Cell
def clean_ner_output(self, outputs):
    """
    Cleaning output for NER task
    """
    results = []
    current = []
    last_idx = 0
    # make to sub group by position
    for output in outputs:
        if output["index"]-1 == last_idx:
            current.append(output)
        else:
            results.append(current)
            current = [output, ]
        last_idx = output["index"]
    if len(current) > 0:
        results.append(current)

    # from tokens to string
    strings = []
    for c in results:
        tokens = []
        starts = []
        ends = []
        for o in c:
            tokens.append(o['word'])
            starts.append(o['start'])
            ends.append(o['end'])

        new_str = self.tokenizer.convert_tokens_to_string(tokens)
        if new_str != '':
            strings.append(dict(
                word=new_str,
                start=min(starts),
                end=max(ends),
                entity=c[0]['entity']
            ))
    return strings

def predict_ner_table(pipeline_kw):
    def predict_ner_table_(self, text: str) -> pd.DataFrame:
        return pd.DataFrame(
            self.clean_output(
                self(text, **pipeline_kw)
                )
            )
    return predict_ner_table_

def refined_ner_pipeline(model, tokenizer, **pipeline_kw):
    if "aggregation_strategy" not in pipeline_kw:
        pipeline_kw["aggregation_strategy"] = "first"

    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer)

    ner_pipeline.__class__.clean_output = clean_ner_output
    ner_pipeline.__class__.predict_table = predict_ner_table(pipeline_kw)
    return ner_pipeline

def refined_ner_from_pretrained(pretrained, **pipeline_kw):
    model = AutoModelForTokenClassification.from_pretrained(pretrained)
    tokenizer = AutoTokenizer.from_pretrained(pretrained)
    return refined_ner_pipeline(model, tokenizer, **pipeline_kw)