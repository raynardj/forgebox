{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parts for hf transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp hf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from forgebox.imports import *\n",
    "from forgebox.category import Category\n",
    "from typing import List, Dict, Callable, Any, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process IOBES files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_iob2_file_to_iobes(file_path, result_path):\n",
    "    \"\"\"\n",
    "    Convert IOB2 file to IOBES\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(result_path, 'w') as f:\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                f.write('\\n')\n",
    "                continue\n",
    "            line = line.split()\n",
    "            if line[-1] == 'O':\n",
    "                f.write(' '.join(line) + '\\n')\n",
    "            else:\n",
    "                f.write(' '.join(line[:-1]) + ' ' + line[-1] + '\\n')\n",
    "\n",
    "\n",
    "def conbine_iobes_file(\n",
    "    file_paths: List[Path],\n",
    "    new_file_path: Path\n",
    "):\n",
    "    \"\"\"\n",
    "    Conbine from multiple IOBES files\n",
    "        into IOBES files\n",
    "    \"\"\"\n",
    "    with open(new_file_path, 'w') as new_file:\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    new_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class IOBES(Dataset):\n",
    "    \"\"\"\n",
    "    Load iobes file for NER training task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path,\n",
    "        tokenizer,\n",
    "        max_len=128,\n",
    "        save_buffer: int = 15,\n",
    "        category: Category = None,\n",
    "        return_string: bool = False,\n",
    "        use_frag: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        file_path,\n",
    "        tokenizer,\n",
    "        max_len=128,\n",
    "        save_buffer: int = 15,\n",
    "        category: Category = None,\n",
    "            label categories, if set to None, will be figured out\n",
    "            automatically.\n",
    "            You can set this to None for train dataset, but for valid\n",
    "            dataset:\n",
    "            valid_ds = IOBES(...,category=train_ds.cates)\n",
    "        return_string: bool = False, do we return original string\n",
    "            for tokenizer output, this option is good for debuging\n",
    "            but the data won't pass into cuda if choose so\n",
    "        use_frag: bool = False, do we use prepend like 'I-','B-'\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.max_len = max_len\n",
    "        self.pairs = []\n",
    "        self.list_of_words = []\n",
    "        self.list_of_labels = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cates = category\n",
    "        self.return_string = return_string\n",
    "        self.use_frag = use_frag\n",
    "        self.load_data(save_buffer)\n",
    "\n",
    "    def load_data(self, save_buffer: int = 15):\n",
    "        \"\"\"\n",
    "        Load file in to object structure\n",
    "        \"\"\"\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    splited = line.split()\n",
    "                    if len(splited) != 2:\n",
    "                        continue\n",
    "                    word, label = splited\n",
    "                    # do we use 'I-', 'B-' etc\n",
    "                    if self.use_frag is False:\n",
    "                        if \"-\" in label:\n",
    "                            label = label.split('-')[1]\n",
    "                    self.pairs.append([word, label])\n",
    "\n",
    "        self.pairs = np.array(self.pairs)\n",
    "\n",
    "        if self.cates is None:\n",
    "            labels_df = pd.DataFrame({\"label\": self.pairs[:, 1]})\n",
    "            self.cates = Category(list(labels_df.vc(\"label\").index))\n",
    "\n",
    "        self.batching_words(save_buffer)\n",
    "\n",
    "    def batching_words(self, save_buffer: int = 15):\n",
    "        \"\"\"\n",
    "        batching self.words into self.list_of_words\n",
    "        by self.max_len -15\n",
    "        \"\"\"\n",
    "        for i in range(0, len(self.pairs), self.max_len-save_buffer):\n",
    "            chunk_slice = slice(i, i+self.max_len-save_buffer)\n",
    "            self.list_of_words.append(self.pairs[chunk_slice, 0])\n",
    "            self.list_of_labels.append(self.pairs[chunk_slice, 1])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.list_of_words)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[str]]:\n",
    "        return list(self.list_of_words[idx]), list(self.list_of_labels[idx])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"NER dataset using IOBES annotation\n",
    "        {len(self)} sentences,\n",
    "        Labels:\n",
    "        {list(self.cates.i2c)}\n",
    "        \"\"\"\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        \"\"\"\n",
    "        data: list of tuple\n",
    "        \"\"\"\n",
    "        words, text_labels = zip(*data)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            list(words),\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        return self.align_offsets(inputs, text_labels, words)\n",
    "\n",
    "    def align_offsets(\n",
    "        self,\n",
    "        inputs,\n",
    "        text_labels: List[List[str]],\n",
    "        words: List[List[str]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inputs: output if tokenizer\n",
    "        text_labels: labels in form of list of list of strings\n",
    "        words: words in form of list of list of strings\n",
    "        \"\"\"\n",
    "        labels = torch.zeros_like(inputs.input_ids).long()\n",
    "        labels -= 100\n",
    "        text_lables_array = np.empty(labels.shape, dtype=object)\n",
    "        words_array = np.empty(labels.shape, dtype=object)\n",
    "        max_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        for row_id, input_ids in enumerate(inputs.input_ids):\n",
    "            word_pos = inputs.word_ids(row_id)\n",
    "            for idx, pos in enumerate(word_pos):\n",
    "                if pos is None:\n",
    "                    continue\n",
    "                if pos <= max_len:\n",
    "                    labels[row_id, idx] = self.cates.c2i[text_labels[row_id][pos]]\n",
    "                    if self.return_string:\n",
    "                        text_lables_array[row_id,\n",
    "                                          idx] = text_labels[row_id][pos]\n",
    "                        words_array[row_id, idx] = words[row_id][pos]\n",
    "\n",
    "        inputs['labels'] = labels\n",
    "        if self.return_string:\n",
    "            inputs['text_labels'] = text_lables_array.tolist()\n",
    "            inputs['word'] = words_array.tolist()\n",
    "        return inputs\n",
    "\n",
    "    def dataloader(self, batch_size: int = 32, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Create dataloader\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def one_batch(self, batch_size: int = 32, shuffle: bool = True):\n",
    "        return next(iter(self.dataloader(batch_size, shuffle)))\n",
    "\n",
    "    def visualize_batch(self, batch, row_idx=0):\n",
    "        return list(zip(self.tokenizer.convert_ids_to_tokens(batch.input_ids[row_idx]),\n",
    "                        batch.labels[row_idx].numpy(),\n",
    "                        batch.text_labels[row_idx],\n",
    "                        batch.word[row_idx],\n",
    "                        batch.offset_mapping[row_idx].numpy(),\n",
    "                        ))\n",
    "\n",
    "    def set_hfconfig(self, config):\n",
    "        \"\"\"\n",
    "        set the category information to huggingface config\n",
    "        \"\"\"\n",
    "        config.num_labels = len(self.cates)\n",
    "        config.id2label = {i: label for i, label in enumerate(self.cates.i2c)}\n",
    "        config.label2id = {label: i for i, label in enumerate(self.cates.i2c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"raynardj/roberta-pubmed\", add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IOBES(\"/Users/xiaochen.zhang/data/valid.iobes\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-O\n",
      "blood-O\n",
      ";-O\n",
      "content-O\n",
      "of-O\n",
      "cAMP-O\n",
      "was-O\n",
      "also-O\n",
      "decreased-O\n",
      "in-O\n",
      "lymphocytes-O\n",
      "by-O\n",
      "33-O\n",
      "%-O\n",
      ".-O\n",
      "At-O\n",
      "the-O\n",
      "same-O\n",
      "time-O\n",
      ",-O\n",
      "total-O\n",
      "content-O\n",
      "of-O\n",
      "T-cell_type\n",
      "lymphocytes-cell_type\n",
      "was-O\n",
      "decreased-O\n",
      "1.5-fold-O\n",
      "in-O\n",
      "peripheric-O\n",
      "blood-O\n",
      ".-O\n",
      "Treatment-O\n",
      "with-O\n",
      "I-hydroxyvitamin-O\n",
      "D3-O\n",
      "(-O\n",
      "1-1.5-O\n",
      "mg-O\n",
      "daily-O\n",
      ",-O\n",
      "within-O\n",
      "4-O\n",
      "weeks-O\n",
      ")-O\n",
      "led-O\n",
      "to-O\n",
      "normalization-O\n",
      "of-O\n",
      "total-O\n",
      "and-O\n",
      "ionized-O\n",
      "form-O\n",
      "of-O\n",
      "Ca2+-O\n",
      "and-O\n",
      "of-O\n",
      "25-O\n",
      "(-O\n",
      "OH-O\n",
      ")-O\n",
      "D-O\n",
      ",-O\n",
      "but-O\n",
      "did-O\n",
      "not-O\n",
      "affect-O\n",
      "the-O\n",
      "PTH-O\n",
      "content-O\n",
      "in-O\n",
      "blood-O\n",
      ".-O\n",
      "Concentration-O\n",
      "of-O\n",
      "the-O\n",
      "receptors-protein\n",
      "to-O\n",
      "1.25-O\n",
      "(-O\n",
      "OH-O\n",
      ")-O\n",
      "2D3-O\n",
      "was-O\n",
      "elevated-O\n",
      "up-O\n",
      "to-O\n",
      "39.7-O\n",
      "fmole/mg-O\n",
      "after-O\n",
      "I-O\n",
      "week-O\n",
      "of-O\n",
      "the-O\n",
      "treatment-O\n",
      ",-O\n",
      "whereas-O\n",
      "it-O\n",
      "was-O\n",
      "decreased-O\n",
      "to-O\n",
      "the-O\n",
      "initial-O\n",
      "level-O\n",
      "24.8-O\n",
      "fmole/mg-O\n",
      "within-O\n",
      "4-O\n",
      "weeks-O\n",
      ";-O\n",
      "simultaneous-O\n",
      "alteration-O\n",
      "in-O\n"
     ]
    }
   ],
   "source": [
    "for w,l in zip(*dataset[2]):\n",
    "    print(f\"{w}-{l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   19,  3741,  2603,  ...,  1417,  2617, 11576],\n",
       "        [ 4590,  2156,   255,  ...,   405,  1182,  6608],\n",
       "        [ 6214, 25683,  3809,  ...,    11,     5,  8151],\n",
       "        ...,\n",
       "        [13998, 25326,  2413,  ...,     5,  2199,    21],\n",
       "        [11299,   705, 24811,  ...,   134,  1589,  2032],\n",
       "        [ 5804,   924,    14,  ...,   366,  1168,     9]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'offset_mapping': tensor([[[ 1,  4],\n",
       "         [ 1,  2],\n",
       "         [ 2,  5],\n",
       "         ...,\n",
       "         [ 3,  5],\n",
       "         [ 5,  8],\n",
       "         [ 1,  6]],\n",
       "\n",
       "        [[ 1,  5],\n",
       "         [ 1,  1],\n",
       "         [ 1,  1],\n",
       "         ...,\n",
       "         [ 5,  7],\n",
       "         [ 7,  9],\n",
       "         [ 9, 14]],\n",
       "\n",
       "        [[ 1,  5],\n",
       "         [ 5,  8],\n",
       "         [ 8, 10],\n",
       "         ...,\n",
       "         [ 1,  2],\n",
       "         [ 1,  3],\n",
       "         [ 1, 10]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1,  5],\n",
       "         [ 5,  8],\n",
       "         [ 8, 10],\n",
       "         ...,\n",
       "         [ 1,  3],\n",
       "         [ 1,  7],\n",
       "         [ 1,  3]],\n",
       "\n",
       "        [[ 1,  5],\n",
       "         [ 5,  6],\n",
       "         [ 6, 10],\n",
       "         ...,\n",
       "         [ 2,  3],\n",
       "         [ 1,  1],\n",
       "         [ 1,  2]],\n",
       "\n",
       "        [[ 1,  7],\n",
       "         [ 1,  5],\n",
       "         [ 1,  4],\n",
       "         ...,\n",
       "         [ 3,  5],\n",
       "         [ 5,  7],\n",
       "         [ 1,  2]]]), 'labels': tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "        [2, 0, 2,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 2, 0, 2],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
