{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA GPU Management\n",
    "> A handler on GPU management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handler on a single CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ftorch.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool will find the CUDA info from the system, the most current info about\n",
    "* How much is the total CUDA memory\n",
    "* How much will the CUDA memory is **in use**\n",
    "* How much CUDA memory is **free**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "class CudaDevice(object):\n",
    "    def __init__(self, idx):\n",
    "        \"\"\"\n",
    "        idx: int, cuda device id\n",
    "        d = CudaDevice(0)\n",
    "        print(d)\n",
    "        \"\"\"\n",
    "        self.idx = idx\n",
    "        self.device = torch.device(idx)\n",
    "        if hasattr(self, \"used\") == False:\n",
    "            self.refresh()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Device {self.idx}: \\n\\tname:{self.prop.name}\\n\\tused:{self.used}MB\\tfree:{self.free}MB\"\n",
    "\n",
    "    @property\n",
    "    def prop(self):\n",
    "        \"\"\"\n",
    "        property on cuda device\n",
    "        \"\"\"\n",
    "        return torch.cuda.get_device_properties(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.prop.total_memory)\n",
    "\n",
    "    @property\n",
    "    def mem(self):\n",
    "        return f\"{self.__len__() // 1024 // 1024} MB\"\n",
    "\n",
    "    def __int__(self):\n",
    "        return int(self.idx)\n",
    "\n",
    "    def refresh(self):\n",
    "        gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "        stat = list(\n",
    "            int(v) for v in str(gpu_stats).split(\"\\\\n\")[1 + self.idx].replace(\" MiB\", \"\").replace(\" \", \"\").split(\",\"))\n",
    "        setattr(self, \"used\", stat[0])\n",
    "        setattr(self, \"free\", stat[1])\n",
    "        return stat\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A handler managing multiple devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class CudaHandler(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ch = CudaHandler()\n",
    "        dev = ch.idle()()\n",
    "        some_torch_tensor.to(dev)\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available() == False:\n",
    "            return None\n",
    "        self.counts = torch.cuda.device_count()\n",
    "        print(f\">>> {self.counts} cuda devices found >>>\")\n",
    "        self.devices = list()\n",
    "        for i in range(self.counts):\n",
    "            d = CudaDevice(i)\n",
    "            self.devices.append(d)\n",
    "            print(f\"{d}\")\n",
    "            setattr(self, f\"gpu{i}\", d)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        counts of the cuda devices numbers\n",
    "        \"\"\"\n",
    "        return self.counts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.devices[idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        self.refresh()\n",
    "        return f\"<{self.counts} cuda devices>\\n\" + \"\\n\".join(list(str(d) for d in (self.devices)))\n",
    "\n",
    "    def refresh(self):\n",
    "        \"\"\"\n",
    "        refresh the cuda mem stats\n",
    "        \"\"\"\n",
    "        gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "        rows = list(list(int(v) for v in vs.replace(\" MiB\", \"\").replace(\" \", \"\").split(\",\")) for vs in\n",
    "                    str(gpu_stats).split(\"\\\\n\")[1:-1])\n",
    "        for i in range(len(rows)):\n",
    "            setattr(self.devices[i], \"used\", rows[i][0])\n",
    "            setattr(self.devices[i], \"free\", rows[i][1])\n",
    "        print(\"cuda stats refreshed\")\n",
    "\n",
    "    def idle(self):\n",
    "        \"\"\"\n",
    "        find the most idle cuda device\n",
    "        \"\"\"\n",
    "        self.refresh()\n",
    "        rt = self[0]\n",
    "        for d in self.devices:\n",
    "            if d.free > rt.free:\n",
    "                rt = d\n",
    "        print(f\"Found the most idle GPU: cuda:{rt.idx}, {rt.free} MB Mem remained\")\n",
    "        return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These handler is to simplify the multi-cuda situation, where you can allocate the most idle GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgebox.ftorch.cuda import CudaHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substantiate the handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = CudaHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found the most idle device, by measure of CUDA memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = ch.idle()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we use the device in pytorch coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_torch_tensor.to(dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
